import sys
import os
sys.path.append('../models')

from msae_relevance_generator import MSAETransformerExplainabilityIntegration
from transformers import CLIPProcessor, CLIPModel
from sae import SAE
import numpy as np
import torch

print("ğŸš€ æµ‹è¯•Relevance Map...")

# 1. åŠ è½½æ‰€æœ‰æ¨¡å‹
print("ğŸ“¥ åŠ è½½æ¨¡å‹...")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# åŠ è½½CLIP
clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

# åŠ è½½SAE
sae_path = "../models/cache/models--WolodjaZ--MSAE/snapshots/e6a85249444be9b25c7751e9677464dedfdc7307/ViT-L_14/centered/6144_768_TopKReLU_64_RW_False_False_0.0_cc3m_ViT-L~14_train_image_2905936_768.pth"
sae_model = SAE(sae_path)
if hasattr(sae_model, 'to'):
    sae_model = sae_model.to(device)

# åŠ è½½æ¦‚å¿µåˆ†æ•°å’Œè¯å…¸
concept_path = "../models/cache/models--WolodjaZ--MSAE/snapshots/e6a85249444be9b25c7751e9677464dedfdc7307/ViT-L_14/centered/Concept_Interpreter_6144_768_TopKReLU_64_RW_False_False_0.0_cc3m_ViT-L~14_train_image_2905936_768_disect_ViT-L~14_-1_text_20000_768.npy"
concept_scores = np.load(concept_path)

vocab_path = "../models/cache/models--WolodjaZ--MSAE/snapshots/e6a85249444be9b25c7751e9677464dedfdc7307/clip_disect_20k.txt"
with open(vocab_path, 'r') as f:
    vocab = [line.strip() for line in f.readlines()]

print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")


# 3. ä½¿ç”¨ä½ å·²ç»åŠ è½½çš„æ¨¡å‹åˆ›å»ºåˆ†æå™¨
analyzer = MSAETransformerExplainabilityIntegration(
   clip_model=clip_model,           # ä½ å·²æœ‰çš„
   clip_processor=clip_processor,   # ä½ å·²æœ‰çš„  
   sae_model=sae_model,            # ä½ å·²æœ‰çš„
   concept_scores=concept_scores,   # ä½ å·²æœ‰çš„
   vocab=vocab,                     # ä½ å·²æœ‰çš„
   device=device
)

# 4. ä¸€é”®ç”Ÿæˆæ‰€æœ‰ç»“æœ
from PIL import Image

test_image_path = "../data/images/Cat/two_cat.png"
image = Image.open(test_image_path).convert('RGB')

results = analyzer.generate_comprehensive_analysis(image)

# # 2. åˆ›å»ºgradient relevance mapç”Ÿæˆå™¨
# generator = GradientRelevanceMapGenerator(
#     clip_model=clip_model,
#     clip_processor=clip_processor, 
#     sae_model=sae_model,
#     concept_scores=concept_scores,
#     vocab=vocab
# )

# # 3. æµ‹è¯•ä¸åŒæ¦‚å¿µ
# test_concepts_mapping = {
#     "Cat": "cat",           # ç²—æ¦‚å¿µ
#     "Siamese": "siamese",   # ç»†æ¦‚å¿µ  
#     "Persian": "persian",   # ç»†æ¦‚å¿µ
#     "British_Shorthair": "cat"  # ç”¨é€šç”¨æ¦‚å¿µ
# }

# print("\nğŸ¯ å¼€å§‹æµ‹è¯•ä¸åŒæ¦‚å¿µçš„relevance map...")

# for folder_concept, vocab_concept in test_concepts_mapping.items():
#     folder_path = f"../data/images/{folder_concept}"
#     if os.path.exists(folder_path):
#         images = [f for f in os.listdir(folder_path) 
#                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
#         if images:
#             # æµ‹è¯•ç¬¬ä¸€å¼ å›¾ç‰‡
#             test_image = os.path.join(folder_path, images[0])
#             print(f"\nğŸ–¼ï¸  æµ‹è¯•: {test_image} -> æ¦‚å¿µ: {vocab_concept}")
            
#             # ç”Ÿæˆrelevance map
#             result = generator.generate_relevance_map(test_image, vocab_concept)
            
#             if result:
#                 print("âœ… Gradient relevance mapç”ŸæˆæˆåŠŸï¼")
                
#                 # ä¿å­˜ç»“æœ
#                 os.makedirs("../outputs", exist_ok=True)
#                 save_path = f"../outputs/gradient_relevance_{folder_concept}_{vocab_concept}.png"
#                 generator.visualize_relevance(result, save_path)
                
#             else:
#                 print("âŒ Relevance mapç”Ÿæˆå¤±è´¥")
                
#             # æ¸…ç†GPUå†…å­˜
#             torch.cuda.empty_cache()

# print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
# print("ğŸ“‚ ç»“æœä¿å­˜åœ¨ ../outputs/ æ–‡ä»¶å¤¹ä¸­")